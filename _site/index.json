[
{
	"uri": "http://christopherleeds.com/aboutme/",
	"title": "About Me",
	"tags": [],
	"description": "",
	"content": "  Background I was born and raised in a small community in Illinois. After high school, I attended Truman State University starting in the fall of 2008, shortly after the housing bubble burst. I quickly developed an interest in economics \u0026amp; finance which led me to complete my BS in economics in 2012.\nAs completion of my undergraduate degree neared, I learned that my skillset was lacking the technical expertise required for adequate consideration in the job market. So, I applied and was accepted to the University of Missouri graduate program in statistics where I would pursue my MA.\nIn Columbia, I developed a strong interest in programming and analytics. The MA statistics program gave me the opportunity to begin developing a much needed technical skillset while teaching a few courses a year.\nUpon graduation, I was able to leverage this newfound interest into a career in research and development. I\u0026rsquo;m always looking to meet and work with motivated and curious individuals.\n Philosophy  Evolve \u0026amp; adapt, be a self-driven \u0026amp; life-long learner. Break down complex problems into their simplest components.\n Deploy rapidly, get feedback faster. Document, document, document!   Skills I practice data science in the biotech \u0026amp; agriculture space. I love what I do and am always looking to grow.\nPlease view my resume for more details.\n"
},
{
	"uri": "http://christopherleeds.com/python/titanic/",
	"title": "Surviving the Titanic",
	"tags": [],
	"description": "test",
	"content": "  Introduction One of the most popular data science competitions on kaggle asks users to predict survival from passenger information from the RMS titanic. A description of the data can be found here.\nWe want to be able to classify any individual passenger as 1=\u0026lsquo;Survived\u0026rsquo; or 0=\u0026lsquo;Not Survived\u0026rsquo; based on the other features in the dataset. Obviously, predicting survival rate on the titanic provides no practical application for future use. But, models can also provide information on the importance of the inputs(features). In this way, we can assess the relative contribution of any individual characteristic in predicting survival.\n Libraries As always, Let\u0026rsquo;s start be bringing in our favorite libraries required for data wrangling \u0026amp; analysis. We need pandas and numpy for wrangling and processing, seaborn \u0026amp; matplotlib for plotting, \u0026amp; sklearn for analytics.\nimport pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt # plotting options %matplotlib inline sns.set_style(\u0026#39;whitegrid\u0026#39;) # for model tuning \u0026amp; fitting from sklearn.ensemble import GradientBoostingClassifier from sklearn.model_selection import GridSearchCV  We are going to use the GridSearchCV function to optimize a our GradientBoostingClassifier. By estimating model accuracy over a grid of possible model parameters, we can select the best choice from amongst a series of gradient boosted classifiers.\n Import Data we can use the pandas read_csv function to read in our datasets for processing.\ntrain0 = pd.read_csv(\u0026quot;train.csv\u0026quot;) test0 = pd.read_csv(\u0026quot;test.csv\u0026quot;)  I find it best to keep tabs on each version of our dataset through process and prevent overwriting. So, we start at version-0 for each dataset, (e.g. test0 \u0026amp; train0).\n Exploring our Data  \u0026ldquo;You can see a lot just by observing.\u0026rdquo; - Yogi Berra\n We can see the description of our data with variables notes here. Our training and testing datasets contain passenger level data where we want to predict on Survived where 1 = Survived and 0 = Did not Survive.\ntrain0.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.6+ KB  We have missing data in Age, Cabin \u0026amp; Embarked. Also, we\u0026rsquo;ll need to convert each of of our character variables to factors before modeling fitting with sklearn.\nAfter reviewing the variable descriptions, it seems like the Ticket \u0026amp; Name variables may not be very useful. Other kernels on kaggle seem to suggest that we can extract some useful information from name by taking the title out of the names. A \u0026lsquo;title\u0026rsquo; variable would probably get\u0026rsquo;s it\u0026rsquo;s value from being proxy for sex and social class.\nBut, Let\u0026rsquo;s explore some of our other features before extracting new features. Below are some basic plots to explore the relationships between gender, ticket class, port of embarktion, age, \u0026amp; fare against survival rate.\n.\nFrom the plots above, we can see that sex will be a large contributor to survival rate. Also, odds of survival increase with ticket class. Futhermore, passengers that embarked at port C survived seem to have higher survival rates. However, there does not seem to be a visible difference between Age \u0026amp; Fare densities with respect to our response.\n Feature Engineering Let\u0026rsquo;s move ahead and extract some meaning from our Name \u0026amp; Cabin variables. First, let\u0026rsquo;s combine our test and training datasets. We should do this to ensure we are processing the data in the same way.\ndatalist = [train0,test0] alldata = pd.concat(datalist) alldata.shape  (1309, 12)  We can utilize the str.extract functionality from the pandas library to extract the first occurrence of a regular expression pattern. This reduces the time needed to manually inspect and extract each title from the Name feature.\nalldata[\u0026#39;title\u0026#39;] = alldata.Name.str.extract(\u0026#39; ([A-Za-z]+)\\.\u0026#39;, expand=False) alldata.groupby(\u0026#39;title\u0026#39;).size()  title Capt 1 Col 4 Countess 1 Don 1 Dona 1 Dr 8 Jonkheer 1 Lady 1 Major 2 Master 61 Miss 260 Mlle 2 Mme 1 Mr 757 Mrs 197 Ms 2 Rev 8 Sir 1 dtype: int64  After extracting our titles, let\u0026rsquo;s \u0026lsquo;bin\u0026rsquo; the less common titles into a Rare category. Then, let\u0026rsquo;s use the seaborn package to see if our new feature may have some value in modeling.\nalldata[\u0026#39;title\u0026#39;] = alldata[\u0026#39;title\u0026#39;].replace([\u0026#39;Mme\u0026#39;,\u0026#39;Lady\u0026#39;,\u0026#39;Countess\u0026#39;,\u0026#39;Capt\u0026#39;,\u0026#39;Col\u0026#39;\\ ,\u0026#39;Don\u0026#39;,\u0026#39;Dr\u0026#39;,\u0026#39;Major\u0026#39;,\u0026#39;Rev\u0026#39;,\u0026#39;Sir\u0026#39;,\u0026#39;Jonkheer\u0026#39;,\u0026#39;Dona\u0026#39;],\u0026#39;Rare\u0026#39;) plotthis = alldata1[~alldata1.Survived.isnull()] plot = sns.barplot(x=\u0026quot;title\u0026quot;,y=\u0026quot;Survived\u0026quot;,data=plotthis); plt.xlabel(\u0026#39;Passenger Title\u0026#39;) plt.ylabel(\u0026#39;Mean Survival Rate\u0026#39;)  We can see significant differences in the survival rate amongst our new feature. This plot suggests that title may provide some value as we craft our models.\nFinally, let\u0026rsquo;s transform our character features into factors for the model fitting process \u0026amp; change the null values.\n# change to factors alldata1[\u0026#39;title\u0026#39;] = alldata1[\u0026#39;title\u0026#39;].map({\u0026quot;Mr\u0026quot;: 1, \u0026quot;Miss\u0026quot;: 2, \u0026quot;Mrs\u0026quot;: 3, \u0026quot;Master\u0026quot;: 4, \u0026quot;Rare\u0026quot;: 5}) alldata1[\u0026#39;Sex\u0026#39;] = alldata1[\u0026#39;Sex\u0026#39;].map({\u0026quot;male\u0026quot;: 1, \u0026quot;Female\u0026quot;: 2}) alldata1[\u0026quot;Embarked\u0026quot;] = alldata1[\u0026#39;Embarked\u0026#39;].map({\u0026quot;S\u0026quot;: 1, \u0026quot;C\u0026quot;: 2, \u0026quot;Q\u0026quot;: 3}) # change missing values to 0 alldata1[\u0026#39;title\u0026#39;] = alldata1[\u0026#39;title\u0026#39;].fillna(0) alldata1[\u0026#39;Sex\u0026#39;] = alldata1[\u0026#39;Sex\u0026#39;].fillna(0) alldata1[\u0026#39;Embarked\u0026#39;] = alldata1[\u0026#39;Embarked\u0026#39;].fillna(0) alldata1[\u0026#39;Age\u0026#39;] = alldata1[\u0026#39;Age\u0026#39;].fillna(0) alldata1[\u0026#39;Fare\u0026#39;] = alldata1[\u0026#39;Fare\u0026#39;].fillna(0)   Model Tuning Now that we are through the wrangling, exploration and feature engineering phases, we can train our models. First, we split our data apart for training and testing.\n# split test and train train1 = alldata1[~alldata1.Survived.isnull()].drop(\u0026quot;PassengerId\u0026quot;,1) test1 = alldata1[alldata1.Survived.isnull()] # split features and response X = train1.drop(\u0026quot;Survived\u0026quot;,1) Y = train1[\u0026quot;Survived\u0026quot;] testX = test1.drop([\u0026#39;Survived\u0026#39;,\u0026#39;PassengerId\u0026#39;],1)  We want to search for the best model fit over a range of parameters. When performing a grid search for optimal parameter values, we should be aware that this procedure is very computationally intensive. In essence, we are fitting the same type of model many times over a 3-fold cross-validation for each combination of parameters and selecting parameters with that provide the best accuracy score.\nparams = {\u0026#39;n_estimators\u0026#39;:range(300,1201,100), \u0026#39;learning_rate\u0026#39;:[0.1,0.05,0.02], \u0026#39;max_depth\u0026#39;:[2,3], \u0026#39;max_features\u0026#39;:[3,4,5]} grdsearch = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=params, scoring=\u0026#39;accuracy\u0026#39;, iid=False, cv=3) searchresults = grdsearch.fit(X,Y)  once the grid search is complete, we can inspect the attributes of our fit object searchresults for the optimal parameters for prediction.\nprint(\u0026quot;Best score acheived is:\u0026quot;,searchresults.best_score_) print(\u0026quot;Best grid parameters for this score are:\u0026quot;) print(searchresults.best_params_)  Best score acheived is: 0.842873176207 Best grid parameters for this score are: {'learning_rate': 0.1, 'max_depth': 3, 'max_features': 3, 'n_estimators': 300}  Since we are utilizing 3-fold cross-validation, we can avoid selecting the best model based on the training error. This is a useful measure to avoid over-fitting.\n Final Prediction Now that we have our parameters tuned from the grid search, we can extract predictions from our model fit and output the results for submission to kaggle. We need to do this in order to get the our test error rate.\ngbc = GradientBoostingClassifier(n_estimators=300, learning_rate=0.1, max_depth=3, max_features=3) model = gbc.fit(X,Y) survival_prediction = model.predict(testX) survival_id = test1[\u0026#39;PassengerId\u0026#39;]  The gradient boosting model has a feature_importances_ attribute which we can extract for plotting with column names. Below is a barplot of the relative importance for each of our features.\nFinally, we can use the to_csv functionality to output or predictions on the test set and submit the results for review.\nsubmitFile = pd.DataFrame({ \u0026#39;PassengerId\u0026#39;: survival_id, \u0026#39;Survived\u0026#39;: survival_prediction }) submitFile.to_csv(\u0026quot;titanicsubmission.csv\u0026quot;, index=False)  When submitting this file to kaggle for submission, the test error rate equals 0.77512.\n"
},
{
	"uri": "http://christopherleeds.com/r/",
	"title": "Data Science with R",
	"tags": [],
	"description": "",
	"content": " Introduction This series of posts will leverage the R language with to perform data wrangling and analysis tasks. My primary experience in data analysis programming is with the R languauge with a workflow revolving around Rstudio functionality combined with plugins for github.\n Current Version R.version.string  \u0026quot;R version 3.3.1 (2016-06-21)\u0026quot;  "
},
{
	"uri": "http://christopherleeds.com/aboutme/resume/",
	"title": "My Resume",
	"tags": [],
	"description": "",
	"content": "\n \n"
},
{
	"uri": "http://christopherleeds.com/_header/",
	"title": "logo",
	"tags": [],
	"description": "",
	"content": "Christopher Leeds\n"
},
{
	"uri": "http://christopherleeds.com/python/",
	"title": "Data Science with Python",
	"tags": [],
	"description": "",
	"content": " Introduction This series of posts will seeks to make show data wrangling, visualization, and analysis with python. I am fairly new to the python programming language and workflow and I look forward to sharing my journey as climb the \u0026lsquo;learning curve\u0026rsquo;.\nOn OSX, I have a homebrew installation of python3 without disturbing our original python 2.7 distribution needed for local apps. My primary workflow will rely on jupyter notebooks so I can organize my code and thoughts.\n Current Version import sys print(sys.version)  3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]  "
},
{
	"uri": "http://christopherleeds.com/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": " Hello! Welcome to my personal website. I built this space as a way to participate and collaborate in the data science community. This site is being rendered with hugo, a great tool to generate static websites quickly.\nIf you have any questions, please feel free to contact me.\n What you\u0026rsquo;ll find Data science is a very broad space and it can be difficult to assess where an individual\u0026rsquo;s strengths lie. Each tool in your toolkit (e.g. git, R, python, plotly, shiny, dash, domino, jenkins, etc.) can become a universe unto itself. So, it becomes important to ask what you can do with these tools.\nWhat can you do with R? How can you deploy python apps/modules? These questions are ultimately more meaningful. And by making this site, I hope to answer that for myself, and encourage others to do the same.\n What is data science?  \u0026ldquo;A data scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician.\u0026rdquo;\n Data science is a large space with people of exceedingly disparate backgrounds. The Data Science Venn Diagram is the most common answer one might hear to the question: what is data science?\nThis image was retrieved from here and is Creative Commons licensed as Attribution-NonCommercial.\nQuite often, I come across data scientists who have built entirely different workflows based on entirely different objectives. My own journey has taken me from a primarily SAS-based workflow with classical statistical models to a workflow based in R with an emphasis on cloud computing tools to automate data wrangling \u0026amp; analysis.\n To Do build layout with hugo create check list post first python entry post first R entry post first machine learning entry post first bayesian analysis entry post first spatial analysis entry post first dashboard creation entry  "
},
{
	"uri": "http://christopherleeds.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://christopherleeds.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]